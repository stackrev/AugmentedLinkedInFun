{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkedIn Profile Analysis\n",
    "\n",
    "We will build a supervised pipeline to classify a profile depending on a subset of tagged data.\n",
    "Beyond the standard word cleanupfor an NLP pipeline, we should:\n",
    "- remove hashtag, both word and symbol.\n",
    "- remove email addresses.\n",
    "- Symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"./anonLinkedInProfiles.csv\"\n",
    "data = pd.concat([chunk for chunk in tqdm(pd.read_csv(DATA, chunksize=1000), desc=f'Loadin {DATA}')])\n",
    "print(f'Shape: {data.shape}, does it have NAs:\\n{data.isna().any()}')\n",
    "\n",
    "data = data.dropna()\n",
    "data = data.drop(data[(data['descriptions'] == '') | (data['titles'] == '')].index)\n",
    "\n",
    "print(f'Post fill NAs:\\n{data.isna().any()}')\n",
    "data['class'] = data['class'].apply(lambda x: x.lower())\n",
    "\n",
    "# For this exercise, keep it small.\n",
    "data = data.sample(500)\n",
    "data = data.reset_index() # Reset index, since we will do operations on it!\n",
    "print(f'Resampled Shape: {data.shape}')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet \n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "NGRAMS = (2,2) # BGrams only\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\", \"|\", \"#\"]\n",
    "COMMON_WORDS = [] # to be populated later in our analysis\n",
    "toktok = ToktokTokenizer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def _get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Creating our tokenizer function. Can also use a TFIDF\n",
    "def custom_tokenizer(sentence):\n",
    "    # Let's use some speed here.\n",
    "    tokens = [toktok.tokenize(sent) for sent in sent_tokenize(sentence)]\n",
    "    tokens = [wnl.lemmatize(word, _get_wordnet_pos(word)) for word in tokens[0]]\n",
    "    tokens = [word.lower().strip() for word in tokens]\n",
    "    tokens = [tok for tok in tokens if (tok not in STOP_WORDS and tok not in SYMBOLS and tok not in COMMON_WORDS)]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    if (type(text) == str):\n",
    "        text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "        text = text.lower()\n",
    "    else:\n",
    "        text = \"NA\"\n",
    "    return text\n",
    "\n",
    "\n",
    "# With the BoW, the model does a bit better.\n",
    "bow_vector = CountVectorizer(\n",
    "    tokenizer=custom_tokenizer, ngram_range=NGRAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Combine features for NLP.\n",
    "X = data['titles'].astype(str) +  ' ' + data['descriptions'].astype(str)\n",
    "ylabels = le.fit_transform(data['class'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3) # no need for strat, doesn't reflect reality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the word frequencies by label, using our vectorizer - this is a slow process, but we want to see if the material for embeddings makes sense for the given class.\n",
    "\n",
    "Anything that is too common, and irrelavant will be added to the COMMONWORDS list, and will be dropped in the vectorization step when building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "def get_top_n_dependant_ngrams(corpus, corpus_labels, ngram=1, n=3):\n",
    "    # use a private vectorizer.\n",
    "    _vect = CountVectorizer(tokenizer=custom_tokenizer,\n",
    "                            ngram_range=(ngram, ngram))\n",
    "    vect = _vect.fit(tqdm(corpus, \"fn:fit\"))\n",
    "    bow_vect = vect.transform(tqdm(corpus, \"fn:transform\"))\n",
    "    features = bow_vect.toarray()\n",
    "\n",
    "    labels = np.unique(corpus_labels)\n",
    "    ngrams_dict = {}\n",
    "    for label in tqdm(labels, \"fn:labels\"):\n",
    "        corpus_label_filtered = corpus_labels == label\n",
    "        features_chi2 = chi2(features, corpus_label_filtered)\n",
    "        feature_names = np.array(_vect.get_feature_names_out())\n",
    "\n",
    "        feature_rev_indices = np.argsort(features_chi2[0])[::-1]\n",
    "        feature_rev_indices = feature_rev_indices[:n]\n",
    "        ngrams = [(feature_names[idx], features_chi2[0][idx]) for idx in feature_rev_indices]\n",
    "        ngrams_dict[label] = ngrams\n",
    "\n",
    "    # while we are at it, let's return top N counts also\n",
    "    sum_words = bow_vect.sum(axis=0)\n",
    "    bottom_words_counts = [(word, sum_words[0, idx])\n",
    "                  for word, idx in tqdm(_vect.vocabulary_.items())]\n",
    "    top_words_counts = sorted(\n",
    "        bottom_words_counts, key=lambda x: x[1], reverse=True)\n",
    "    top_words_counts = top_words_counts[:n]\n",
    "    bottom_words_counts= bottom_words_counts[:n]\n",
    "        \n",
    "    return {'labels_freq': ngrams_dict,\n",
    "            'top_corpus_freq': top_words_counts,\n",
    "            'bottom_corpus_freq': bottom_words_counts}\n",
    "\n",
    "\n",
    "TOP_N_WORDS = 10\n",
    "\n",
    "common_bigrams_label_dict = get_top_n_dependant_ngrams(X, ylabels, ngram=1, n=TOP_N_WORDS)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(26, 12), sharey=False)\n",
    "fig.suptitle('NGrams per Class')\n",
    "fig.subplots_adjust(hspace=0.25, wspace=0.50)\n",
    "\n",
    "x_plot = 0\n",
    "y_plot = 0\n",
    "labels = np.sort(np.unique(ylabels), axis=None)\n",
    "for idx, label in tqdm(enumerate(labels), \"Plot labels\"):\n",
    "    common_ngrams_df = pd.DataFrame(\n",
    "        common_bigrams_label_dict['labels_freq'][label], columns=['ngram', 'chi2'])\n",
    "    x1, y1 = common_ngrams_df['chi2'], common_ngrams_df['ngram']\n",
    "\n",
    "    # Reverse it from the ordinal label we transformed it.\n",
    "    axes[y_plot][x_plot].set_title(\n",
    "        f'{le.inverse_transform([label])} ngram dependence', fontsize=6)\n",
    "    axes[y_plot][x_plot].set_yticklabels(y1, rotation=0)\n",
    "    sns.barplot(ax=axes[y_plot][x_plot], x=x1, y=y1)\n",
    "    # Go to next plot.\n",
    "    if idx > 0 and idx % 2 == 0:\n",
    "        x_plot = 0\n",
    "        y_plot += 1\n",
    "    else:\n",
    "        x_plot += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(common_bigrams_label_dict['top_corpus_freq'])\n",
    "print(common_bigrams_label_dict['bottom_corpus_freq'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These most frequent or least frequent words don't have relevance to the individual classes, and are available in all text. We check if they these don't have high chi2 score though, as we don't want to alter the text's classification by removing a highly correlated but low frequency word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_label_freq = [word for label in labels for word, count in common_bigrams_label_dict['labels_freq'][label]]\n",
    "print(f'Highest frequency of ngrames in labels: {common_label_freq}')\n",
    "\n",
    "COMMON_WORDS = np.append([word for word,count in common_bigrams_label_dict['top_corpus_freq'] if word not in common_label_freq], \n",
    "                         [word for word,count in common_bigrams_label_dict['bottom_corpus_freq'] if word not in common_label_freq ])\n",
    "COMMON_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,2))\n",
    "sns.countplot(x=y_train)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our training dataset is unbalanced, we need to resample to least common class, or use weights. The former is the easiest for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "keys = np.unique(y_train)\n",
    "values = compute_class_weight(class_weight='balanced', classes=keys, y=y_train)\n",
    "\n",
    "class_weights = dict(zip(keys, values))\n",
    "print(f'Use these wieghts: {class_weights}')\n",
    "\n",
    "# Or undersmaple.\n",
    "\n",
    "min_size = np.array([len(data[data['class'] == 's']), len(data[data['class'] == 'o']), len(data[data['class'] == 'c']), len(data[data['class'] == 'f']), len(data[data['class'] == 'w'])]).min()\n",
    "print(f'Least sampled class of size {min_size}')\n",
    "\n",
    "data4 = data[data['class'] == 's'].sample(n=min_size, random_state=101)\n",
    "data3 = data[data['class'] == 'o'].sample(n=min_size, random_state=101)\n",
    "data2 = data[data['class']=='c'].sample(n=min_size, random_state=101)\n",
    "data1 = data[data['class']=='f'].sample(n=min_size, random_state=101)\n",
    "data0 = data[data['class']=='w'].sample(n=min_size, random_state=101)\n",
    "\n",
    "data_under = pd.concat([data0,data1,data2,data3,data4],axis=0)\n",
    "\n",
    "print(f'Undersampled shapes: {data0.shape}, {data1.shape}, {data2.shape}, {data3.shape}, {data4.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model here. We will use a pipeline, we a preselected classifier (in this case an SVM case the best results in previous tests), and cross validate the best hyperparams. We then save the model for reuse later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# If we want to use an ensemle in case of weak models:\n",
    "# \n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import StackingClassifier\n",
    "# estimators = [ ('lsv', LinearSVC()), ('sgdc', SGDClassifier())]\n",
    "# sclf = StackingClassifier(estimators=estimators,\n",
    "#                          final_estimator=LogisticRegression(),\n",
    "#                          passthrough=True)\n",
    "\n",
    "text_clf = Pipeline([\n",
    "        (\"cleaner\", predictors()),\n",
    "        ('vect', bow_vector),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', LinearSVC()),\n",
    "    ],\n",
    "    verbose=False) # Add verbose to see progress, note that we run x2 for each param combination.\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 2)],\n",
    "    'tfidf__use_idf': [True],\n",
    "    'tfidf__sublinear_tf': [True],\n",
    "    'clf__penalty': ['l2'],\n",
    "    'clf__loss':  ['squared_hinge'],\n",
    "    'clf__C': [1],\n",
    "    'clf__class_weight': ['balanced']\n",
    "}\n",
    "model_clf = GridSearchCV(text_clf,\n",
    "                        param_grid=parameters,\n",
    "                        refit=True,\n",
    "                        cv=2,\n",
    "                        error_score='raise')\n",
    "model = model_clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring and analysing our model. We look at the best hyperparams our CV has supplied, for the next model build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy\n",
    "print(\"F1:\", metrics.f1_score(y_test, predicted, average='weighted'))\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Precision:\", metrics.precision_score(\n",
    "    y_test, predicted, average='weighted'))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, predicted, average='weighted'))\n",
    "\n",
    "# see: model.cv_results_ for more reuslts\n",
    "print(f'The best estimator: {model.best_estimator_}\\n')\n",
    "print(f'The best score: {model.best_score_}\\n')\n",
    "print(f'The best parameters: {model.best_params_}\\n')\n",
    "\n",
    "#Ploting the confusion matrix\n",
    "plt.figure(figsize=(2, 2))\n",
    "cm = metrics.confusion_matrix(y_test, predicted)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                      display_labels=model.classes_)\n",
    "\n",
    "disp.plot()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and test the model. This pickled model will be used with a server. We do the same for the label encoder. Remember to copy these to the server.\n",
    "\n",
    "We can look into quantization for reduced model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)\n",
    "\n",
    "pickled_le = dump(le, './models/labelencoder.joblib')\n",
    "validate_pickled_le = load('./models/labelencoder.joblib')\n",
    "pickled_model = dump(model, './models/model.joblib')\n",
    "validate_pickled_model = load('./models/model.joblib')\n",
    "\n",
    "xx_test = [\"IT recruitment Consultant at SNFHL I'm an IT/SAP/CRYPTO recruiter, who likes to learn new stuff and tries some basic coding with web3 and SQL in my free time. Crypto-bro for life!\"]\n",
    "\n",
    "yy_resultt = validate_pickled_model.predict(xx_test)\n",
    "validate_pickled_le.inverse_transform(yy_resultt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
